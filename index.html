<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <!-- Meta tags for social media banners, these should be filled in appropriatly as they are your "business card" -->
  <!-- Replace the content tag with appropriate information -->
  <meta name="description" content="DESCRIPTION META TAG">
  <meta property="og:title" content="SOCIAL MEDIA TITLE TAG"/>
  <meta property="og:description" content="SOCIAL MEDIA DESCRIPTION TAG TAG"/>
  <meta property="og:url" content="URL OF THE WEBSITE"/>
  <!-- Path to banner image, should be in the path listed below. Optimal dimenssions are 1200X630-->
  <meta property="og:image" content="static/image/your_banner_image.png" />
  <meta property="og:image:width" content="1200"/>
  <meta property="og:image:height" content="630"/>


  <meta name="twitter:title" content="TWITTER BANNER TITLE META TAG">
  <meta name="twitter:description" content="TWITTER BANNER DESCRIPTION META TAG">
  <!-- Path to banner image, should be in the path listed below. Optimal dimenssions are 1200X600-->
  <meta name="twitter:image" content="static/images/your_twitter_banner_image.png">
  <meta name="twitter:card" content="summary_large_image">
  <!-- Keywords for your paper to be indexed by-->
  <meta name="keywords" content="KEYWORDS SHOULD BE PLACED HERE">
  <meta name="viewport" content="width=device-width, initial-scale=1">


  <title>Academic Project Page</title>
  <link rel="icon" type="image/x-icon" href="static/images/favicon.ico">
  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro"
  rel="stylesheet">

  <link rel="stylesheet" href="static/css/bulma.min.css">
  <link rel="stylesheet" href="static/css/bulma-carousel.min.css">
  <link rel="stylesheet" href="static/css/bulma-slider.min.css">
  <link rel="stylesheet" href="static/css/fontawesome.all.min.css">
  <link rel="stylesheet"
  href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="static/css/index.css">

  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script src="https://documentcloud.adobe.com/view-sdk/main.js"></script>
  <script defer src="static/js/fontawesome.all.min.js"></script>
  <script src="static/js/bulma-carousel.min.js"></script>
  <script src="static/js/bulma-slider.min.js"></script>
  <script src="static/js/index.js"></script>
</head>
<body>


  <section class="hero">
    <div class="hero-body">
      <div class="container is-max-widescreen">
        <div class="columns is-centered">
          <div class="column has-text-centered">
            <h1 class="title is-1 publication-title">NeuroSync: Intent-Aware Code-Based Problem Solving via Direct LLM Understanding Modification</h2>
            <div class="is-size-5 publication-authors">
              <!-- Paper authors -->
              <span class="author-block">
                <a href="https://zhang373.github.io/" target="_blank">Wenshuo Zhang</a><sup>*</sup>,</span>
                <span class="author-block">
                  <a href="https://shenleixian.github.io/" target="_blank">Leixian Shen</a><sup></sup>,</span>
                <span class="author-block">
                  <a href="https://shuchangxu.github.io/" target="_blank">Shuchang Xu</a><sup></sup>,</span>
                <span class="author-block">
                  <a href="https://hippodu001.github.io/" target="_blank">Jindu Wang</a><sup></sup>,</span>
                <span class="author-block">
                  <a href="https://www.jeffjianzhao.com/" target="_blank">Jian Zhao</a><sup></sup>,</span>
                <span class="author-block">
                  <a href="http://www.huamin.org/" target="_blank">Huamin Qu</a><sup></sup>,</span>
                 <span class="author-block">
                    <a href="https://yuanlinping.top/" target="_blank">Lin-Ping Yuan</a><sup>+</sup>
                  </span>
                  </div>

                  <div class="is-size-5 publication-authors">
                    <span class="author-block">The Hong Kong University of Science and Technology<br>UIST '25: The 38th Annual ACM Symposium on User Interface Software and Technology</span>
                    <span class="eql-cntrb"><small><br><sup>*</sup>wzhangeb@connect.ust.hk, who is activly looking for research intern position</small></span>
                    <span class="eql-cntrb"><small><br><sup>+</sup>corrsponding author</small></span>
                  </div>

                  <div class="column has-text-centered">
                    <div class="publication-links">
                         <!-- Arxiv PDF link -->
                      <span class="link-block">
                        <a href="https://hkustconnect-my.sharepoint.com/:b:/g/personal/wzhangeb_connect_ust_hk/EfUShav0dB9EpZZPZX0h-8YByrlPGb-oTp2oZlz5FVIFDQ?e=sHpkbA" target="_blank"
                        class="external-link button is-normal is-rounded is-dark">
                        <span class="icon">
                          <i class="fas fa-file-pdf"></i>
                        </span>
                        <span>Paper</span>
                      </a>
                    </span>

                    <!-- Supplementary PDF link -->
                    <span class="link-block">
                      <a href="https://hkustconnect-my.sharepoint.com/:p:/g/personal/wzhangeb_connect_ust_hk/ER6Km4ctdbhHlYRitqsnVi0BMMVJjw1CzoXYasevbiEL2w?e=tFuKBf" target="_blank"
                      class="external-link button is-normal is-rounded is-dark">
                      <span class="icon">
                        <i class="fas fa-file-pdf"></i>
                      </span>
                      <span>Slide</span>
                    </a>
                  </span>

                  <!-- Github link -->
                  <span class="link-block">
                    <a href="https://github.com/zhang373/NeuroSync" target="_blank"
                    class="external-link button is-normal is-rounded is-dark">
                    <span class="icon">
                      <i class="fab fa-github"></i>
                    </span>
                    <span>Code</span>
                  </a>
                </span>

                  <!-- Github link -->
                  <span class="link-block">
                    <a href="10.1145/3746059.3747668" target="_blank"
                    class="external-link button is-normal is-rounded is-dark">
                    <span class="icon">
                      <i class="fas fa-file-pdf"></i>
                    </span>
                    <span>DOI</span>
                  </a>
                </span>

                <!-- ArXiv abstract Link -->
                <span class="link-block">
                  <a href="https://arxiv.org/abs/<ARXIV PAPER ID>" target="_blank"
                  class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                    <i class="ai ai-arxiv"></i>
                  </span>
                  <span>arXiv</span>
                </a>
              </span>
            </div>
          </div>
        </div>
      </div>
    </div>
  </div>
</section>


<!-- Teaser video-->
<!-- <section class="hero teaser">
  <div class="container is-max-desktop">
    <div class="hero-body">
      <video poster="" id="tree" autoplay controls muted loop height="100%">

        <source src="static/videos/banner_video.mp4"
        type="video/mp4">
      </video>
      <h2 class="subtitle has-text-centered">
        Aliquam vitae elit ullamcorper tellus egestas pellentesque. Ut lacus tellus, maximus vel lectus at, placerat pretium mi. Maecenas dignissim tincidunt vestibulum. Sed consequat hendrerit nisl ut maximus. 
      </h2>
    </div>
  </div>
</section> -->
<!-- End teaser video -->

<!-- Paper abstract -->
<section class="section hero is-light">
  <div class="container is-max-desktop">
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Abstract</h3>
        <div class="content has-text-justified">
          <p>
            Conversational LLMs have been widely adopted by domain users with limited programming experience to solve domain problems. However, these users often face misalignment between their intent and generated code, resulting in frustration and rounds of clarification. This work first investigates the cause of this misalignment, which dues to bidirectional ambiguity: both user intents and coding tasks are inherently nonlinear, yet must be expressed and interpreted through linear prompts and code sequences. To address this, we propose direct intent–task matching, a new human–LLM interaction paradigm that externalizes and enables direct manipulation of the LLM understanding, i.e., the coding tasks and their relationships inferred by the LLM prior to code generation. As a proof-of-concept, this paradigm is then implemented in NeuroSync, which employs a knowledge distillation pipeline to extract LLM understanding, user intents, and their mappings, and enhances the alignment by allowing users to intuitively inspect and edit them via visualizations. We evaluate the algorithmic components of NeuroSync via technical experiments, and assess its overall usability and effectiveness via a user study (N=12). The results show that it enhances intent–task alignment, lowers cognitive effort, and improves coding efficiency.
          </p>
        </div>
      </div>
    </div>
  </div>
</section>
<!-- End paper abstract -->



<!-- Image carousel -->
<section class="hero is-small">
  <div class="hero-body">
    <div class="container">
      <h2 class="title is-3">Why Misalignment and How to Solve (Formative Study)</h2>
      <div class="content has-text-centered">
        <!-- <p class="subtitle is-5 has-text-justified">
          To examine the causes of human–LLM misalignment in conversational coding and inform system design, we conducted a formative study with domain users who have little coding experiences. The study included (1) interaction history analysis and interviews to uncover misalignment patterns, and (2) semi-structured interviews, informed by a literature review, to explore effective representations of graphs for conveying code tasks and user intent. 
        </p> -->
        <p class="subtitle is-5 has-text-justified">
          Bidirectional ambiguity is one important reason why misalignment occurs and graphs (node-link diagrams) are a good link between users's nonlinear intent and LLM's nonlinear code tasks.
        </p>
      </div>
      <div id="results-carousel" class="carousel results-carousel">
       <div class="item">
        <!-- Your image here -->
        <div style="text-align: center; margin: 20px 0;">
          <img src="static/images/formative.png" alt="Formative Study" style="width: 50%; height: auto;"/>
        </div>
        <div class="subtitle has-text-justified" style="max-width: 1200px; margin: 0 auto;">
          <p style="margin-bottom: 1rem;">
            <strong>Why misalignment? </strong>
          </p>
          <p style="margin-bottom: 1rem;">
            Bidirectional ambiguity is a major cause of human-LLM misalignment in conversational coding tasks. During the conversation with LLM for coding tasks, ambiguity is bidirectional:
          </p>
          <p style="margin-bottom: 1rem;">
            <strong>(1) User-to-LLM:</strong> Users find it challenging to clearly express their needs and the information required by the LLM in their prompts. For example, converting tree-like intent in Fig. into prompt will lose direct structure and cause ambiguity.
          </p>
          <p style="margin-bottom: 1rem;">
            <strong>(2) LLM-to-User:</strong> Users struggle to understand the specific tasks and execution logic embedded in the code, making it difficult to provide precise modification requests. For example, in Fig., users need to reconstruct codes and code relationships by themselves, which is difficult and low ability of understanding code will lead to ambiguity.
          </p>
          <p>
            This bidirectional ambiguity compounds over turns, causing LLMs to produce code misaligned with user intent. As LLM capabilities grow and inference slows, the cost of these ineffective interactions increases.
          </p>
        </div>
      </div>
       <div class="item">
        <!-- Your image here -->
        <div style="text-align: center; margin: 20px 0;">
          <img src="static/images/phase2.png" alt="Formative Study" style="width: 80%; height: auto;"/>
        </div>
        <div class="subtitle has-text-justified" style="max-width: 1200px; margin: 0 auto;">
          <p style="margin-bottom: 1rem;">
            <strong>How to cross the dual nonlinearity?</strong>
          </p>
          <p style="margin-bottom: 1rem;">
            task graphs were consistently viewed as more helpful. Participants highlighted two key benefits:(1) Improved task comprehension through clear visualization of task dependencies and subgoals;  (2) Enhanced efficiency in locating key logic points and understanding overall code purpose.
          </p>
          <p>
            However, as interaction rounds increased, graph complexity grew and negatively impacted interpretability. System need dynamic graph simplification methods.
          </p>
        </div>
      </div>
  </div>
</div>
</div>
</section>
<!-- End image carousel -->


<!-- Image carousel -->
<section class="hero is-small">
  <div class="hero-body">
    <div class="container">
      <h2 class="title is-3">LLM Understanding and Intent-Task Direct Matching (Concept)</h2>
      <div class="content has-text-centered">
        <p class="subtitle is-5 has-text-justified">
          Inspired by the concept of <strong>understanding</strong>, where humans develop their interpretation of LLM outputs, we suggest that LLMs form a kind of understanding of user inputs. We call this <strong>LLM understanding</strong>, which refers to the tasks and their relationships implicitly encoded in the code that an LLM is expected to generate based on user prompts.        </p>
        <p class="subtitle is-5 has-text-justified">
          We propose a new human–LLM interaction paradigm, <strong>direct intent–task matching</strong>, based on externalizing and modifying LLM understanding organized in graphs prior to code generation.
        </p>
      </div>
      
      <!-- Single image -->
      <div style="text-align: center; margin: 20px 0;">
        <img src="static/images/concept.png" alt="Concept" style="width: 60%; height: auto;"/>
      </div>
    </div>
  </div>
</section>
<!-- End image carousel -->

<!-- Image carousel -->
<!-- <section class="hero is-small">
  <div class="hero-body">
    <div class="container">
      <h2 class="title is-3">Overview of NeuroSync</h2>
      <div class="content has-text-centered">
        <p class="subtitle is-5 has-text-justified">
Overview of \tool, a proof-of-concept implementation of the <strong>direct intent–task matching</strong> paradigm. \tool takes user prompts as input, extracts the <em>LLM understanding</em>, enables users to refine this understanding through graph-based visualizations, and feeds the refined understanding back to the LLM to generate code that more accurately aligns with user intents.        </p>
      </div>
      

      <div style="text-align: center; margin: 20px 0;">
        <img src="static/images/system_overview.png" alt="Concept" style="width: 60%; height: auto;"/>
      </div>
    </div>
  </div>
</section> -->
<!-- End image carousel -->


<!-- Image carousel -->
<section class="hero is-small">
  <div class="hero-body">
    <div class="container">
      <h2 class="title is-3">NeuroSync</h2>
      <div class="content has-text-centered">
        <!-- <p class="subtitle is-5 has-text-justified">
          To examine the causes of human–LLM misalignment in conversational coding and inform system design, we conducted a formative study with domain users who have little coding experiences. The study included (1) interaction history analysis and interviews to uncover misalignment patterns, and (2) semi-structured interviews, informed by a literature review, to explore effective representations of graphs for conveying code tasks and user intent. 
        </p> -->
        <p class="subtitle is-5 has-text-justified">
<strong>NeuroSync</strong> allows a user to directly manipulate a visual task graph on two levels via the <strong>user interface</strong> to correct an LLM's understanding before code generation. This interaction is kept responsive by a lightweight <strong>distillation pipeline</strong>, which fine-tunes a small model using data from a <strong>multi-agent system</strong> that simulates user behavior. To manage cognitive load, an <strong>intent-aware graph simplification algorithm</strong> dynamically collapses and highlights parts of the graph based on the user's focus. </p>
      </div>
      <div id="results-carousel" class="carousel results-carousel">
       <div class="item">
        <!-- Your image here -->
        <div style="text-align: center; margin: 20px 0;">
          <img src="static/images/system_overview.png" alt="Formative Study" style="width: 50%; height: auto;"/>
        </div>
        <div class="subtitle has-text-justified" style="max-width: 1200px; margin: 0 auto;">
          <p style="margin-bottom: 1rem;">
            <strong>Overview of NeuroSync</strong>, a proof-of-concept implementation of the <strong>direct intent–task matching</strong> paradigm. NeuroSync takes user prompts as input, extracts the <em>LLM understanding</em>, enables users to refine this understanding through graph-based visualizations, and feeds the refined understanding back to the LLM to generate code that more accurately aligns with user intents.        
          </p>
        </div>
      </div>
       <div class="item">
        <!-- Your image here -->
        <div style="text-align: center; margin: 20px 0;">
          <img src="static/images/interface_final_big_v1.png" alt="Formative Study" style="width: 50%; height: auto;"/>
        </div>
        <div class="subtitle has-text-justified" style="max-width: 1200px; margin: 0 auto;">
          <p style="margin-bottom: 1rem;">
<strong>User interaction</strong> in NeuroSync supports two-level graph modification before code generation. Users can perform precise node-level edits (e.g., adding/modifying nodes) or use natural language commands for broad, graph-level structural changes.
        </div>
      </div>

       <div class="item">
        <!-- Your image here -->
        <div style="text-align: center; margin: 20px 0;">
          <img src="static/images/usage_jindu_extrem_large_v2.png" alt="Formative Study" style="width: 50%; height: auto;"/>
        </div>
        <div class="subtitle has-text-justified" style="max-width: 1200px; margin: 0 auto;">
          <p style="margin-bottom: 1rem;">
            <strong>Interface of NeuroSync.</strong> Users interact with the LLM through Panel A (LLM Conversation Panel). Before each LLM response, the system generates an LLM understanding graph in Panel B (Understanding Graph Manipulation Panel) and a simplified version in Panel C (Intent–Task Mapping View). Users can edit the task graph in Panel B and explore task structures and intent alignment via Panel C.
          </p>
        </div>
      </div>

      <div class="item">
        <!-- Your image here -->
        <div style="text-align: center; margin: 20px 0;">
          <img src="static/images/pipeline.png" alt="Formative Study" style="width: 50%; height: auto;"/>
        </div>
        <div class="subtitle has-text-justified" style="max-width: 1200px; margin: 0 auto;">
          <p style="margin-bottom: 1rem;">
            <strong>Triple Distillation Pipeline.</strong> It aligns the SLM in the student path with the two-stage extractor in the teacher path. The SLM can extract triples directly from prompts, bypassing intermediate code generation to speed up triple extraction.
          </p>
        </div>
      </div>

        <div class="item">
        <!-- Your image here -->
        <div style="text-align: center; margin: 20px 0;">
          <img src="static/images/4_Reply_Pipeline.png" alt="Formative Study" style="width: 50%; height: auto;"/>
        </div>
        <div class="subtitle has-text-justified" style="max-width: 1200px; margin: 0 auto;">
          <p style="margin-bottom: 1rem;">
            <strong>Multi-Agent Module Overview.</strong> This module involves four agents designed to interact with each other, simulating a domain user's experience of leveraging an LLM for code generation based on our findings on user behavior patterns.
          </p>
        </div>
      </div>

        <div class="item">
        <!-- Your image here -->
        <div style="text-align: center; margin: 20px 0;">
          <img src="static/images/graph_sim_v3.png" alt="Formative Study" style="width: 50%; height: auto;"/>
        </div>
        <div class="subtitle has-text-justified" style="max-width: 1200px; margin: 0 auto;">
          <p style="margin-bottom: 1rem;">
            <strong>Intent-aware graph simplification algorithm.</strong> The left figure illustrates an intent tree, where each node corresponds to a sub-understanding graph. During the simplification process, nodes that are mapped to changes in the intent tree are directly transferred to the simplified graph (\ie~red dashed box). Meanwhile, parts mapped to unchanged nodes are recursively collapsed or zoomed out (\ie~blue and green boxes).
          </p>
        </div>
      </div>

      </div>
  </div>
</div>
</div>
</section>
<!-- End image carousel -->

<!-- Youtube video -->
<section class="hero is-small is-light">
  <div class="hero-body">
    <div class="container">
      <!-- Paper video. -->
      <h2 class="title is-3">Usage Scenario Video</h3>
      <div class="columns is-centered has-text-centered">
        <div class="column is-four-fifths">
          
          <div class="publication-video">
            <!-- Youtube embed code here -->
            <iframe src="https://www.youtube.com/embed/aM1DQt1hK0U" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe>
          </div>
        </div>
      </div>
    </div>
  </div>
</section>
<!-- End youtube video -->




<!--BibTex citation -->
  <section class="section" id="BibTeX">
    <div class="container is-max-desktop content">
      <h2 class="title">BibTeX</h2>
      <pre><code>BibTex Code Here</code></pre>
    </div>
</section>
<!--End BibTex citation -->


  <footer class="footer">
  <div class="container">
    <div class="columns is-centered">
      <div class="column is-8">
        <div class="content">

          <p>
            This page was built using the <a href="https://github.com/eliahuhorwitz/Academic-project-page-template" target="_blank">Academic Project Page Template</a> which was adopted from the <a href="https://nerfies.github.io" target="_blank">Nerfies</a> project page.
            You are free to borrow the source code of this website, we just ask that you link back to this page in the footer. <br> This website is licensed under a <a rel="license"  href="http://creativecommons.org/licenses/by-sa/4.0/" target="_blank">Creative
            Commons Attribution-ShareAlike 4.0 International License</a>.
          </p>

        </div>
      </div>
    </div>
  </div>
</footer>

<!-- Statcounter tracking code -->
  
<!-- You can add a tracker to track page visits by creating an account at statcounter.com -->

    <!-- End of Statcounter Code -->

  </body>
  </html>
